{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "ZctkwS4dmwwB",
        "colab_type": "code",
        "outputId": "e5cd0e1e-57d2-43e4-c4a5-f615d19bd2cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM, Convolution1D, Flatten, Dropout\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "50q6Sx0orli-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For this analysis we’ll be using a dataset of 50,000 movie reviews taken from IMDb. The data was compiled by Andrew Maas and can be found here: [IMDb Reviews.](http://ai.stanford.edu/~amaas/data/sentiment/)\n",
        "\n",
        "The data is split evenly with 25k reviews intended for training and 25k for testing your classifier. Moreover, each set has 12.5k positive and 12.5k negative reviews.\n",
        "\n",
        "IMDb lets users rate movies on a scale from 1 to 10. To label these reviews the curator of the data labeled anything with ≤ 4 stars as negative and anything with ≥ 7 stars as positive. Reviews with 5 or 6 stars were left out.\n",
        "\n",
        "Ofcourse, we can download it and do the pre-processing ourselves, well well well Keras already provides a wrapper over pre-processed dataset."
      ]
    },
    {
      "metadata": {
        "id": "7f1URxjBm8TE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "top_words = 10000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eJ1eayclnCyZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# These words are encoded into numbers by the index of their place in the vocabulary.\n",
        "print (X_train[0], y_train[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-SkdDd12ywNz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Let us decode a sentence to see what it holds\n",
        "d = imdb.get_word_index()\n",
        "inv_map = {v: k for k, v in d.items()}\n",
        "for i in X_train[0][1:]:\n",
        "  if (i>=3):\n",
        "    print (inv_map[i-3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "llqmZra5nKIL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "max_review_length = 80 # ? You need to change this, maybe 100, maybe 1000, how do you find it? Maybe the max length of reviews or maybe something for faster training! Do you always need to compromise in life?\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=#?? # What if we don't pad the test data? "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VrUzgUvXnOEL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print (X_train[0], len(X_train[0]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4QcA32Sns9r9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![Conv1D](https://cdn-images-1.medium.com/max/1600/1*aBN2Ir7y2E-t2AbekOtEIw.png)\n"
      ]
    },
    {
      "metadata": {
        "id": "D6yffxb2nQ9r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embedding_vector_length = 300\n",
        "model = Sequential()\n",
        "model.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length))\n",
        "# Some bad architecture choice here, Could you please try to find what's wrong?\n",
        "model.add(Convolution1D(64, 3, padding='same')) # Why padding is same? https://keras.io/layers/convolutional/\n",
        "model.add(Convolution1D(32, 3, padding='same'))\n",
        "model.add(Convolution1D(16, 3, padding='same'))\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.2)) # Should you increase this for more generalization?\n",
        "model.add(Dense(180,activation='sigmoid'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1,activation='sigmoid')) # Can we use 2 here?\n",
        "print (model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5_7xVu1WnTI0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x8G-0F1nnY3u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, nb_epoch=2, verbose = 1, batch_size = 64)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yM4xeesBnm_X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss_acc = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(\"Test data: loss = %0.6f  accuracy = %0.2f%% \" % \\\n",
        "  (loss_acc[0], loss_acc[1]*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c2wjuPSNt_7y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 5. save model\n",
        "print(\"Saving model to disk \\n\")\n",
        "mp = \".\\\\Models\\\\imdb_model.h5\"\n",
        "model.save(mp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eEWW_hN1uLFG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"New review: \\'the movie was waste\\'\")\n",
        "review = \"the movie was waste\"\n",
        "words = review.split()\n",
        "review = []\n",
        "for word in words:\n",
        "  if word not in d: \n",
        "    review.append(2)\n",
        "  else:\n",
        "    review.append(d[word]+3) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HGbLP_50biWb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "review = [0]*(max_review_length-len(review)) + review"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s7-amy-kuY7F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# review = sequence.pad_sequences(review, maxlen=80)\n",
        "import numpy as np\n",
        "review = np.array(review)\n",
        "print (review.shape)\n",
        "prediction = model.predict(review.reshape(1,80))\n",
        "print(\"Prediction (0 = negative, 1 = positive) = \", end=\"\")\n",
        "print(\"%0.4f\" % prediction[0][0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w6jw8WWVzvSe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's  try an LSTM based model, **could you change this to BiLSTM model?**\n",
        "\n",
        "LSTMs and their bidirectional variants are popular because they have tried to learn how and when to forget and when not to using gates in their architecture. In previous RNN architectures, vanishing gradients was a big problem and caused those nets not to learn so much.\n",
        "\n",
        "Using Bidirectional LSTMs, you feed the learning algorithm with the original data once from beginning to the end and once from end to beginning."
      ]
    },
    {
      "metadata": {
        "id": "9rh8ucBUufkO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras as K\n",
        "model = K.models.Sequential()\n",
        "model.add(K.layers.embeddings.Embedding(input_dim=top_words,\n",
        "  output_dim=embedding_vector_length, mask_zero=True))\n",
        "model.add(K.layers.LSTM(units=100, dropout=0.2, recurrent_dropout=0.2))  # 100 memory\n",
        "model.add(K.layers.Dense(units=1, activation='sigmoid'))\n",
        "print (model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e6jae4Dac-tx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vjv7ZfZWdBdV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, nb_epoch=2, verbose = 1, batch_size = 64)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r6lJgPrzdDwV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss_acc = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(\"Test data: loss = %0.6f  accuracy = %0.2f%% \" % \\\n",
        "  (loss_acc[0], loss_acc[1]*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sfmvdBuDeUEA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}