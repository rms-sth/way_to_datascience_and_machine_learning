{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of the lecture \"Image processing and computer vision\" of the [First Nepal Winter School on AI](https://nepalschool.naamii.com.np/)  which is adapted from the material prepared by Bishesh Khanal for \"Mathematics in Computer Vision and Machine Learning\" of the [NPCVML](https://npcvml.org) Winter School 2016.\n",
    "We explore PCA which is one of the very basic and important techniques to understand dimensionality reduction and latent representation.\n",
    "It also lets us one way to look at eigenvectors and eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import misc\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set(style='white', palette='Set2', font_scale=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a synthetic data of height, weight and sex created for tomorrow's machine learning lecture. We will load that data here and explore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_ht = pd.read_csv('weight_height_data.csv')\n",
    "wt_ht.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lmplot('weight', 'height', data=wt_ht, hue='gender', fit_reg=False, height = 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let us forget that there are two groups\n",
    " \n",
    "Let us forget for a while that the data was originally coming from two different groups with different mean height and weight. So we will see only two features weight and height as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lmplot('weight', 'height', data=wt_ht, hue=None, fit_reg=False, height = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert weight and height into numpy array\n",
    "A = wt_ht[['weight', 'height']].values\n",
    "print( A.shape)\n",
    "print(A[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, we have a matrix of size 1000 X 2. Each column is one feature. In this case we have two features: weight and height. Number of rows is equal to the number of data points. We will now see how the eigenvectors and eigenvalues of its covariance matrix tells us more about the data. What we will be doing now is called principal component analysis (PCA). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Computing a covariance matrix $C$\n",
    "\n",
    "### Statistics preliminaries\n",
    "\n",
    "Mean: $\\bar x = \\frac{\\sum_{i=1}^{n}{X_i}}{n}$\n",
    "\n",
    "Variance: $\\mathrm{Var}(x) = \\frac{\\sum_{i=1}^{n}(x_i - \\bar x)^2}{n-1}$\n",
    "\n",
    "Variance provides a measure for how much the data vary over its mean.\n",
    "\n",
    "Covariance of two variables:  $\\mathrm{cov}(x, y) = \\frac{\\sum_{i=1}^{n}(x_i - \\bar x)(y_i - \\bar y)}{n-1}$\n",
    "\n",
    "Covariance provides a measure for how much the two variables vary from mean with respect to each other.\n",
    "\n",
    "### Our weight and height data\n",
    "\n",
    "We have the two variables weight and height as:\n",
    "\n",
    "$A = $ $\n",
    "\\begin{bmatrix}\n",
    "\\text{Weight}(x) & \\text{Height} (y)\\\\\n",
    "\\hline\n",
    "64.31  & 156.69 \\\\\n",
    "47.18  & 158.88 \\\\\n",
    "47.21  & 162.66 \\\\\n",
    "... & ...\n",
    "\\end{bmatrix}\n",
    "$  A matrix of size 1000 X 2\n",
    "\n",
    "Subtract mean from each column to get $D$. Also known as *centering the data*.\n",
    "\n",
    "$D = \\begin{bmatrix}\n",
    "64.31 - \\bar x  & 156.69 - \\bar y\\\\\n",
    "47.18 - \\bar x & 158.88 - \\bar y\\\\\n",
    "47.21 - \\bar x  & 162.66 - \\bar y\\\\\n",
    "... & ...\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "Covariance matrix: \n",
    "\n",
    "$C = \\frac{D^T D}{n-1} = \n",
    "\\begin{bmatrix} \n",
    "\\mathrm{Var}(x) & \\mathrm{Covar}(x,y)\\\\\n",
    "\\mathrm{Covar}(x,y) & \\mathrm{Var}(y)\n",
    "\\end{bmatrix}$ A symmetric matrix of size 2X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean of each column\n",
    "mu = np.mean(A, axis=0)\n",
    "# Center the data\n",
    "D = A - mu\n",
    "# Compute covariance\n",
    "C = (D.T).dot(D) / (A.shape[0]-1)\n",
    "print('Covariance matrix \\n%s' %C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Compute Eigenvalues and Eigenvectors of $C$\n",
    "\n",
    "The eigenvectors of the covariance matrix are know as principal components or directions and the eignevalues are the principal values. Eigenvectors corresponding to the higher values of eigenvalues describe the most part of the data. \n",
    "\n",
    "The biggest principal component describes the largest variance in the data.\n",
    "\n",
    "The eigenvector with lowest eigenvalue describes the least variance in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_vals, eig_vecs = np.linalg.eig(C)\n",
    "print('Eigenvectors \\n%s' %eig_vecs)\n",
    "print('\\nEigenvalues \\n%s' %eig_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Projecting the data into a feature subspace\n",
    "\n",
    "Here we select a certain number of principal components over which to project our data. In our case, we are working with 2D, so let us just project into a line in the direction of eigenvector with the highest eigenvalue.\n",
    "The projection  matrix is created by accumulating each of the selected eigenvectors as a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eigenvecs are kept row wise, convert it to a projection matrix by taking its transpose\n",
    "eig_vecs = eig_vecs.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project the data into all the feature space.\n",
    "projected_data = np.dot(D, eig_vecs)\n",
    "# Mean of the standard deviations in each principal directions\n",
    "sigma = projected_data.std(axis=0).mean()\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(wt_ht['weight'], wt_ht['height'])\n",
    "#ax = sns.lmplot('weight', 'height', data=wt_ht, hue=None, fit_reg=False, size = 7)\n",
    "for axis in eig_vecs:\n",
    "    start, end = mu, mu + sigma * axis \n",
    "    ax.annotate(\n",
    "        '', xy=end, xycoords='data',\n",
    "        xytext=start, textcoords='data',\n",
    "        arrowprops=dict(facecolor='red', width=5.0))\n",
    "ax.set_aspect('equal')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Projecting into a subspace, in our case a line.\n",
    "project_line = np.dot(D, eig_vecs.T[0])\n",
    "wt_ht['pca1'] = project_line\n",
    "#wt_ht['id'] = wt_ht.index.values\n",
    "wt_ht['x'] = len(wt_ht['pca1'])*[0,]\n",
    "#ax = sns.lmplot('weight', 'height', data=wt_ht, hue='gender', fit_reg=False, size = 7)\n",
    "#ax = sns.lmplot('id', 'pca1', data=wt_ht, hue='gender', fit_reg=False, height = 7)\n",
    "ax = sns.lmplot('x', 'pca1', data=wt_ht, hue='gender', fit_reg=False, height = 7)\n",
    "ax.set_xticklabels('0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some notes and resources\n",
    "\n",
    "## Data centering\n",
    "\n",
    "We computed covariance matrix of the data which includes the centering of the data. In other methods, if the data is not centered you need to first do that. These following links are of very good discussion about this:\n",
    "\n",
    "1. http://stats.stackexchange.com/questions/189822/how-does-centering-make-a-difference-in-pca-for-svd-and-eigen-decomposition\n",
    "\n",
    "2. http://stats.stackexchange.com/questions/22329/how-does-centering-the-data-get-rid-of-the-intercept-in-regression-and-pca\n",
    "\n",
    "## Understanding PCA\n",
    "\n",
    "The following link has an answer to a PCA question which explains in a very good way its meaning.\n",
    "\n",
    "http://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
